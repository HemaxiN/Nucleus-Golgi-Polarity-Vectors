{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374070c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "from typing import List, Dict, Tuple\n",
    "import io\n",
    "import sys\n",
    "import pickle\n",
    "import itertools\n",
    "import datetime\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import csv\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torch_geometric\n",
    "import torch_geometric.transforms as T\n",
    "import torch_geometric.transforms\n",
    "import torch_geometric.datasets\n",
    "import torch_geometric.nn\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', module='sklearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cc7a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from env_variables import *\n",
    "from gnn_models import *\n",
    "from utils_helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f73a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_output_dir = \"./results\"\n",
    "if not os.path.exists(results_output_dir):\n",
    "    os.makedirs(results_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf221e7",
   "metadata": {},
   "source": [
    "# Deep Learning Pytorch\n",
    "Packages versions:\n",
    "- torchmetrics==0.5.0\n",
    "- pytorch-lightning==1.5.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202807d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_param_combos_gnn(number_edge_features):\n",
    "    nonlinearity_common = nn.LeakyReLU(inplace=True, negative_slope=0.2)  # nn.Tanh()\n",
    "    use_batchnorm_list = [False]\n",
    "    # Initial edge model\n",
    "    initial_edge_model_input_dim_list = [number_edge_features]#[4]  # fixed\n",
    "    edge_dim_list = [16]  # [16, 32]\n",
    "    fc_dims_initial_edge_model_multipliers_list = [(1, 1)]  # (1, 1)\n",
    "    nonlinearity_initial_edge_list = [nonlinearity_common]\n",
    "\n",
    "    # Initial node model\n",
    "    fc_dims_initial_node_model_multipliers_list = [(2, 4, 1)]  # (1, 2, 1), (2,4,1)\n",
    "    nonlinearity_initial_node_list = [nonlinearity_common]\n",
    "\n",
    "    # Edge model\n",
    "    fc_dims_edge_model_multipliers_list = [(4, 1)]  # (6,4,1), (4,1)\n",
    "    nonlinearity_edge_list = [nonlinearity_common]\n",
    "\n",
    "    # TimeAware Node model\n",
    "    fc_dims_directed_flow_model_multipliers_list = [(2, 1)]  # (4,2,1), (2,1); (4,2,2,1) for online\n",
    "    nonlinearity_directed_flow_list = [nonlinearity_common]\n",
    "    node_model_agg_list = [\"max\"]  # [\"max\", \"attention\", \"attention_classifier\", \"attention_normalized\"]\n",
    "    # multiplies node_dim, last layer output is always 1 [None, (2,1)]\n",
    "    fc_dims_node_attention_model_multipliers_list = [None]\n",
    "\n",
    "    # (4,2,1), (6,4,2,1)  (2,1) online - just an extra MLP\n",
    "    fc_dims_total_flow_model_multipliers_list = [(4, 2, 1)]\n",
    "    nonlinearity_total_flow_list = [nonlinearity_common]\n",
    "\n",
    "    # Edge classification model\n",
    "    fc_dims_edge_classification_model_multipliers_list = [\n",
    "        (4, 2, 1,)]  # (2,1) [(0.5, ), None]  # mutliplies edge_dim\n",
    "    nonlinearity_edge_classification_list = [nonlinearity_common]\n",
    "\n",
    "    mpn_steps_list = [4]#[args.mpn_steps] -> number of message passing steps default = 4\n",
    "    is_recurrent_list = [True]\n",
    "    node_dim_multiplier_list = [2]\n",
    "\n",
    "    use_timeaware_list = [False]\n",
    "    use_same_frame_list = [False]#[not args.no_sameframe] -> default False\n",
    "    use_separate_edge_model_list = [False]\n",
    "    use_initial_node_model_list = [True]\n",
    "    edge_mlps_count_list = [3]#[args.edge_mlps_count] - > Number of distinct node MLPs default = 3\n",
    "    node_aggr_sections_list = [3]#[args.node_aggr_sections] -> \"Number of distinct sections in Node aggregation default=3\n",
    "\n",
    "    param_combos = list(product(initial_edge_model_input_dim_list,\n",
    "                            edge_dim_list, fc_dims_initial_edge_model_multipliers_list, nonlinearity_initial_edge_list,\n",
    "                            fc_dims_initial_node_model_multipliers_list, nonlinearity_initial_node_list,\n",
    "                            node_model_agg_list, fc_dims_node_attention_model_multipliers_list,\n",
    "                            fc_dims_edge_model_multipliers_list, nonlinearity_edge_list,\n",
    "                            fc_dims_directed_flow_model_multipliers_list, nonlinearity_directed_flow_list,\n",
    "                            fc_dims_total_flow_model_multipliers_list, nonlinearity_total_flow_list,\n",
    "                            fc_dims_edge_classification_model_multipliers_list, nonlinearity_edge_classification_list,\n",
    "                            use_batchnorm_list,\n",
    "                            mpn_steps_list, is_recurrent_list, node_dim_multiplier_list,\n",
    "                            use_timeaware_list, use_same_frame_list, use_separate_edge_model_list, use_initial_node_model_list,\n",
    "                            edge_mlps_count_list,\n",
    "                            node_aggr_sections_list,\n",
    "                            ))\n",
    "\n",
    "    return param_combos\n",
    "\n",
    "def build_params_dict_gnn(initial_edge_model_input_dim, edge_dim, fc_dims_initial_edge_model_multipliers, nonlinearity_initial_edge,\n",
    "                       fc_dims_initial_node_model_multipliers, nonlinearity_initial_node, \n",
    "                       directed_flow_agg, fc_dims_directed_flow_attention_model_multipliers,\n",
    "                       fc_dims_edge_model_multipliers, nonlinearity_edge,\n",
    "                       fc_dims_directed_flow_model_multipliers, nonlinearity_directed_flow, \n",
    "                       fc_dims_total_flow_model_multipliers, nonlinearity_total_flow,\n",
    "                       fc_dims_edge_classification_model_multipliers, nonlinearity_edge_classification,\n",
    "                       use_batchnorm: bool,\n",
    "                       mpn_steps: int, is_recurrent: bool, node_dim_multiplier: int,\n",
    "                       use_timeaware: bool, use_same_frame: bool, use_separate_edge_model: bool, use_initial_node_model: bool,\n",
    "                       edge_mlps_count: int,\n",
    "                       node_aggr_sections: int,\n",
    "                       **kwargs,\n",
    "                       ):\n",
    "    # workaround before adding sacred\n",
    "    params = {\n",
    "        \"initial_edge_model_input_dim\": initial_edge_model_input_dim,\n",
    "        \"edge_dim\": edge_dim,\n",
    "\n",
    "        \"fc_dims_initial_edge_model_multipliers\": fc_dims_initial_edge_model_multipliers,\n",
    "        \"nonlinearity_initial_edge\": nonlinearity_initial_edge,\n",
    "\n",
    "        \"fc_dims_initial_node_model_multipliers\": fc_dims_initial_node_model_multipliers,\n",
    "        \"nonlinearity_initial_node\": nonlinearity_initial_node,\n",
    "        \"directed_flow_agg\": directed_flow_agg,\n",
    "        \"fc_dims_directed_flow_attention_model_multipliers\": fc_dims_directed_flow_attention_model_multipliers,\n",
    "\n",
    "        \"fc_dims_edge_model_multipliers\": fc_dims_edge_model_multipliers,\n",
    "        \"nonlinearity_edge\": nonlinearity_edge,\n",
    "\n",
    "        \"fc_dims_directed_flow_model_multipliers\": fc_dims_directed_flow_model_multipliers,\n",
    "        \"nonlinearity_directed_flow\": nonlinearity_directed_flow,\n",
    "\n",
    "        \"fc_dims_total_flow_model_multipliers\": fc_dims_total_flow_model_multipliers,\n",
    "        \"nonlinearity_total_flow\": nonlinearity_total_flow,\n",
    "\n",
    "        \"fc_dims_edge_classification_model_multipliers\": fc_dims_edge_classification_model_multipliers,\n",
    "        \"nonlinearity_edge_classification\": nonlinearity_edge_classification,\n",
    "\n",
    "        \"use_batchnorm\": use_batchnorm,\n",
    "\n",
    "        \"mpn_steps\": mpn_steps,\n",
    "        \"is_recurrent\": is_recurrent,\n",
    "        \"node_dim_multiplier\": node_dim_multiplier,\n",
    "\n",
    "        \"use_timeaware\": use_timeaware,\n",
    "        \"use_same_frame\": use_same_frame,\n",
    "        \"use_separate_edge_model\": use_separate_edge_model,\n",
    "        \"use_initial_node_model\": use_initial_node_model,\n",
    "        \"edge_mlps_count\": edge_mlps_count,\n",
    "        \"node_aggr_sections\": node_aggr_sections,\n",
    "    }\n",
    "    params.update(kwargs)\n",
    "    return params\n",
    "\n",
    "def build_models_gnn(params: Mapping[str, Any]):\n",
    "    use_batchnorm = params[\"use_batchnorm\"]\n",
    "\n",
    "    edge_dim = params[\"edge_dim\"]\n",
    "    node_dim_multiplier = params.get(\"node_dim_multiplier\", 2)\n",
    "    node_dim = edge_dim * node_dim_multiplier  # Have nodes hold 2x info of edges\n",
    "    use_timeaware = params.get(\"use_timeaware\", True)\n",
    "    use_same_frame = params.get(\"use_same_frame\", False)\n",
    "    # separate backward/forward/sameframe MLPs or inter/intraframe or single MLP for all\n",
    "    edge_mlps_count = params.get(\"edge_mlps_count\", 3)\n",
    "    assert edge_mlps_count > 0 and edge_mlps_count <= 3, f\"edge_mlps_count must be 1/2/3, not {edge_mlps_count}\"\n",
    "    node_aggr_sections = params.get(\"node_aggr_sections\", 3)\n",
    "    assert node_aggr_sections > 0 and node_aggr_sections <= 3, f\"node_aggr_sections must be 1/2/3, not {node_aggr_sections}\"\n",
    "    # only makes sense when using intraframe\n",
    "    use_separate_edge_model = use_same_frame and params.get(\"use_separate_edge_model\", False) \n",
    "    use_initial_node_model = params.get(\"use_initial_node_model\", True)\n",
    "\n",
    "    # Edge classification model\n",
    "    fc_dims_edge_classification_model_multipliers = params[\"fc_dims_edge_classification_model_multipliers\"]\n",
    "    if fc_dims_edge_classification_model_multipliers is not None:\n",
    "        fc_dims_edge_classification_model = dims_from_multipliers(\n",
    "            edge_dim, fc_dims_edge_classification_model_multipliers) + (1,)\n",
    "    else:\n",
    "        fc_dims_edge_classification_model = (1,)\n",
    "    edge_classifier = MLP(edge_dim, fc_dims_edge_classification_model,\n",
    "                          params[\"nonlinearity_edge_classification\"], last_output_free=True)\n",
    "\n",
    "    # Initial edge model:\n",
    "    fc_dims_initial_edge = dims_from_multipliers(\n",
    "        edge_dim, params[\"fc_dims_initial_edge_model_multipliers\"])\n",
    "    initial_edge_model = MLP(params[\"initial_edge_model_input_dim\"], fc_dims_initial_edge,\n",
    "                            params[\"nonlinearity_initial_edge\"], use_batchnorm=use_batchnorm)\n",
    "    if use_separate_edge_model:\n",
    "        initial_same_frame_edge_model = MLP(params[\"initial_edge_model_input_dim\"], fc_dims_initial_edge,\n",
    "                                            params[\"nonlinearity_initial_edge\"], use_batchnorm=use_batchnorm)\n",
    "    else:\n",
    "        initial_same_frame_edge_model = None\n",
    "\n",
    "    # Initial node model\n",
    "    if use_initial_node_model:\n",
    "        initial_node_agg_mode = params[\"directed_flow_agg\"]\n",
    "        if \"attention\" in initial_node_agg_mode:\n",
    "            if \"classifier\" in initial_node_agg_mode:\n",
    "                initial_node_attention_model = edge_classifier\n",
    "            else:\n",
    "                fc_dims_directed_flow_attention_model_multipliers = params[\"fc_dims_directed_flow_attention_model_multipliers\"]\n",
    "                if fc_dims_directed_flow_attention_model_multipliers is not None:\n",
    "                    fc_dims_initial_node_attention = dims_from_multipliers(\n",
    "                        edge_dim, fc_dims_directed_flow_attention_model_multipliers) + (1,)\n",
    "                else:\n",
    "                    fc_dims_initial_node_attention = (1,)\n",
    "                initial_node_attention_model = MLP(edge_dim, fc_dims_initial_node_attention,\n",
    "                                                params[\"nonlinearity_initial_node\"], last_output_free=True)\n",
    "        else:\n",
    "            initial_node_attention_model = None\n",
    "\n",
    "        fc_dims_initial_node = dims_from_multipliers(\n",
    "            node_dim, params[\"fc_dims_initial_node_model_multipliers\"])\n",
    "        if use_timeaware:\n",
    "            if use_same_frame:\n",
    "                initial_node_model = InitialContextualNodeModel(MLP(edge_dim * 3, fc_dims_initial_node,\n",
    "                                                                params[\"nonlinearity_initial_node\"], use_batchnorm=use_batchnorm),\n",
    "                                                                initial_node_agg_mode, initial_node_attention_model)\n",
    "            else:\n",
    "                initial_node_model = InitialTimeAwareNodeModel(MLP(edge_dim * 2, fc_dims_initial_node,  # x2 for [forward|backward] edge features\n",
    "                                                                   params[\"nonlinearity_initial_node\"], use_batchnorm=use_batchnorm),\n",
    "                                                               initial_node_agg_mode)\n",
    "        else:\n",
    "            assert not use_same_frame\n",
    "            initial_node_model = InitialUniformAggNodeModel(MLP(edge_dim, fc_dims_initial_node,\n",
    "                                                                params[\"nonlinearity_initial_node\"], use_batchnorm=use_batchnorm),\n",
    "                                                            initial_node_agg_mode)\n",
    "    else:  # initial nodes are zero vectors\n",
    "        initial_node_model = InitialZeroNodeModel(node_dim)\n",
    "\n",
    "    # Define models in MPN\n",
    "    edge_models, node_models = [], []\n",
    "    steps = params[\"mpn_steps\"]\n",
    "    assert steps > 1, \"Fewer than 2 MPN steps does not make sense as in that case nodes do not get a chance to update\"\n",
    "    is_recurrent = params[\"is_recurrent\"]\n",
    "    for step in range(steps):\n",
    "        # Edge model\n",
    "        edge_model_input = node_dim * 2 + edge_dim  # edge_dim * 5\n",
    "        fc_dims_edge = dims_from_multipliers(\n",
    "            edge_dim, params[\"fc_dims_edge_model_multipliers\"])\n",
    "        edge_models.append(BasicEdgeModel(MLP(edge_model_input, fc_dims_edge,\n",
    "                                              params[\"nonlinearity_edge\"], use_batchnorm=use_batchnorm)))\n",
    "\n",
    "        if step == steps - 1: # don't need a node update at the last step\n",
    "            continue\n",
    "\n",
    "        # Node model\n",
    "        flow_model_input = node_dim * 2 + edge_dim  # two nodes and their edge\n",
    "        fc_dims_directed_flow = dims_from_multipliers(\n",
    "            node_dim, params[\"fc_dims_directed_flow_model_multipliers\"])\n",
    "        fc_dims_aggregated_flow = dims_from_multipliers(\n",
    "            node_dim, params[\"fc_dims_total_flow_model_multipliers\"])\n",
    "        \n",
    "        node_agg_mode = params[\"directed_flow_agg\"]\n",
    "        if \"attention\" in node_agg_mode:\n",
    "            if \"classifier\" in node_agg_mode:\n",
    "                attention_model = edge_classifier\n",
    "            else:\n",
    "                fc_dims_directed_flow_attention_model_multipliers = params[\"fc_dims_directed_flow_attention_model_multipliers\"]\n",
    "                if fc_dims_directed_flow_attention_model_multipliers is not None:\n",
    "                    fc_dims_directed_flow_attention = dims_from_multipliers(\n",
    "                        node_dim, fc_dims_directed_flow_attention_model_multipliers) + (1,)\n",
    "                else:\n",
    "                    fc_dims_directed_flow_attention = (1,)\n",
    "                attention_model = MLP(node_dim, fc_dims_directed_flow_attention,\n",
    "                                    params[\"nonlinearity_directed_flow\"], last_output_free=True)\n",
    "        else:\n",
    "            attention_model = None\n",
    "\n",
    "        if use_timeaware:\n",
    "            forward_flow_model = MLP(flow_model_input, fc_dims_directed_flow,\n",
    "                                     params[\"nonlinearity_directed_flow\"], use_batchnorm=use_batchnorm)\n",
    "            if edge_mlps_count < 3:\n",
    "                backward_flow_model = forward_flow_model\n",
    "            else:\n",
    "                backward_flow_model = MLP(flow_model_input, fc_dims_directed_flow,\n",
    "                                        params[\"nonlinearity_directed_flow\"], use_batchnorm=use_batchnorm)\n",
    "            if use_same_frame:\n",
    "                if edge_mlps_count == 1:\n",
    "                    frame_flow_model = forward_flow_model\n",
    "                else:\n",
    "                    frame_flow_model = MLP(flow_model_input, fc_dims_directed_flow,\n",
    "                                        params[\"nonlinearity_directed_flow\"], use_batchnorm=use_batchnorm)\n",
    "                aggregated_flow_model = MLP(node_dim * 3, fc_dims_aggregated_flow,\n",
    "                                            params[\"nonlinearity_total_flow\"], use_batchnorm=use_batchnorm)\n",
    "                node_models.append(ContextualNodeModel(\n",
    "                    forward_flow_model, frame_flow_model, backward_flow_model, aggregated_flow_model, node_agg_mode, attention_model, node_aggr_sections=node_aggr_sections))\n",
    "\n",
    "            else:\n",
    "                aggregated_flow_model = MLP(node_dim * 2, fc_dims_aggregated_flow,\n",
    "                                            params[\"nonlinearity_total_flow\"], use_batchnorm=use_batchnorm)\n",
    "                node_models.append(TimeAwareNodeModel(\n",
    "                    forward_flow_model, backward_flow_model, aggregated_flow_model, node_agg_mode))\n",
    "        else:\n",
    "            individual_flow_model = MLP(flow_model_input, fc_dims_directed_flow,\n",
    "                                        params[\"nonlinearity_directed_flow\"], use_batchnorm=use_batchnorm)\n",
    "            aggregated_flow_model = MLP(node_dim, fc_dims_aggregated_flow,\n",
    "                                        params[\"nonlinearity_total_flow\"], use_batchnorm=use_batchnorm)\n",
    "            node_models.append(UniformAggNodeModel(individual_flow_model,\n",
    "                               aggregated_flow_model, node_agg_mode))\n",
    "\n",
    "        if is_recurrent:  # only one model to use at each step\n",
    "            break\n",
    "\n",
    "    if is_recurrent:\n",
    "        assert len(edge_models) == len(node_models) == 1\n",
    "        if use_separate_edge_model:\n",
    "            same_frame_edge_model = BasicEdgeModel(MLP(edge_model_input, fc_dims_edge, params[\"nonlinearity_edge\"],\n",
    "                                                    use_batchnorm=use_batchnorm))\n",
    "        else:\n",
    "            same_frame_edge_model = None\n",
    "\n",
    "        if use_initial_node_model:\n",
    "            mpn_model = MessagePassingNetworkRecurrent(edge_models[0], node_models[0], steps,\n",
    "                                                    use_same_frame, same_frame_edge_model)\n",
    "        else:  # use a node-to-edge MPN\n",
    "            mpn_model = MessagePassingNetworkRecurrentNodeEdge(edge_models[0], node_models[0], steps,\n",
    "                                                               use_same_frame, same_frame_edge_model)\n",
    "    else:\n",
    "        mpn_model = MessagePassingNetworkNonRecurrent(edge_models, node_models, steps, use_same_frame)\n",
    "\n",
    "    return initial_edge_model, initial_same_frame_edge_model, initial_node_model, mpn_model, edge_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12a0fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphClassifierGNN(torch.nn.Module):\n",
    "    def __init__(self, params: Mapping):\n",
    "        \"\"\" Top level model class holding all components necessary to perform tracking on a graph\n",
    "        :param initial_same_frame_edge_model: a torch model processing initial edge attributes for same frame edges\n",
    "        :param initial_node_model: a torch model processing edge attributes to get initial node features\n",
    "        :param mpn_model: a message passing model\n",
    "        :param edge_classifier: a final classification model operating on final edge features\n",
    "        :param params: params\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.params = params\n",
    "        \n",
    "        #change this because we only one edge model\n",
    "        (self.initial_edge_model, self.initial_same_frame_edge_model, self.initial_node_model,\n",
    "             self.mpn_model, self.edge_classifier) = build_models_gnn(params)\n",
    "        \n",
    "        self.use_same_frame = self.params[\"use_same_frame\"]\n",
    "        self.device = torch.device('cpu')\n",
    "    \n",
    "    def forward(self, data):\n",
    "        edge_index, edge_attr, num_nodes = data.edge_index.long(), data.edge_attr, data.num_nodes\n",
    "        same_frame_edge_index = data.same_frame_edge_index.long() if self.use_same_frame else None\n",
    "        same_frame_edge_attr = data.same_frame_edge_attr if self.use_same_frame else None\n",
    "\n",
    "        # Initial Edge embeddings with Null node embeddings\n",
    "        edge_attr = self.initial_edge_model(edge_attr)\n",
    "        if self.use_same_frame:\n",
    "            if self.initial_same_frame_edge_model is not None:\n",
    "                same_frame_edge_attr = self.initial_same_frame_edge_model(same_frame_edge_attr)\n",
    "            else:\n",
    "                same_frame_edge_attr = self.initial_edge_model(same_frame_edge_attr)\n",
    "        \n",
    "        # Initial Node embeddings with Null original embeddings\n",
    "        x = self.initial_node_model(edge_index, edge_attr, num_nodes,\n",
    "                                    same_frame_edge_index=same_frame_edge_index, \n",
    "                                    same_frame_edge_attr=same_frame_edge_attr, \n",
    "                                    device=self.device)\n",
    "        assert len(x) == num_nodes\n",
    "        \n",
    "        x, final_edge_embeddings = self.mpn_model(x, edge_index, edge_attr, num_nodes,\n",
    "                                                 same_frame_edge_index=same_frame_edge_index,\n",
    "                                                 same_frame_edge_attr=same_frame_edge_attr)\n",
    "        \n",
    "        return self.edge_classifier(final_edge_embeddings)\n",
    "    \n",
    "    def forward_graph(self, graph, criterion = None):\n",
    "        out = self.forward(graph.pyg_graph).view(-1)\n",
    "        loss = None\n",
    "        true = graph.pyg_graph.edge_label\n",
    "        if(criterion):\n",
    "            loss = criterion(out, true)\n",
    "        return out, loss, true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88661c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphClassifierMLP(torch.nn.Module):\n",
    "    def __init__(self, dimensions):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(len(dimensions) - 1):\n",
    "            layers.append(torch.nn.Linear(dimensions[i], dimensions[i+1]))\n",
    "            layers.append(torch.nn.ReLU())  # You can use other activation functions here\n",
    "\n",
    "        # Remove the last ReLU layer\n",
    "        layers.pop()\n",
    "\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "    \n",
    "    def forward_graph(self, graph, criterion = None):\n",
    "        out = self.forward(torch.from_numpy(graph.edge_x).to(torch.float)).view(-1)\n",
    "        true = torch.from_numpy(graph.edge_y)\n",
    "        loss = None\n",
    "        if(criterion):\n",
    "            loss = criterion(out, true)\n",
    "        return out, loss, true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db7bebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_constraints_func(probabilities, data, threshold=0.5, debug = False):\n",
    "    # Apply constraints to the predicted probabilities\n",
    "    edge_types = data.edge_types.detach().cpu().numpy()\n",
    "    source_nodes = data.edge_index[0].detach().cpu().numpy()\n",
    "    target_nodes = data.edge_index[1].detach().cpu().numpy()\n",
    "    \n",
    "    # Sort edge_list_info and probabilities in descending order of probabilities\n",
    "    sorted_indices = sorted(range(len(probabilities)), key=lambda k: probabilities[k], reverse=True)\n",
    "    \n",
    "    edge_types_sorted = [edge_types[i] for i in sorted_indices]\n",
    "    source_nodes_sorted = [source_nodes[i] for i in sorted_indices]\n",
    "    target_nodes_sorted = [target_nodes[i] for i in sorted_indices]\n",
    "    \n",
    "    probabilities_sorted = [probabilities[i] for i in sorted_indices]\n",
    "\n",
    "    # Create a set to keep track of assigned node ids\n",
    "    assigned_nodes = set()\n",
    "\n",
    "    # Create a new list to store the predicted labels\n",
    "    pred_labels = [0] * len(probabilities)\n",
    "    \n",
    "    allowed_edge_types = set([edges_type_int_encodings['nuclei-golgi'], edges_type_int_encodings['golgi-nuclei']])\n",
    "    \n",
    "    # Assign 1 to the links with the highest probabilities for each nuclei and golgi\n",
    "    if(debug):\n",
    "        print(\"\\n\\n\\nConstraints \", threshold, \"\\n\\n\")\n",
    "    \n",
    "    for i in range(len(sorted_indices)):\n",
    "        src = source_nodes_sorted[i]\n",
    "        tgt = target_nodes_sorted[i]\n",
    "        edge_type = edge_types_sorted[i]\n",
    "        prob = probabilities_sorted[i]\n",
    "        if(debug):\n",
    "            print(\"src:\",src, \"tgt:\",tgt, \"prob:\",prob, \"assigned: \", end = \"\")\n",
    "        \n",
    "        # If the edge is nuclei-golgi or golgi-nuclei and the nodes are not already assigned and probability > threshold\n",
    "        if (edge_type in allowed_edge_types) and (src not in assigned_nodes) and (tgt not in assigned_nodes):\n",
    "\n",
    "            if prob > threshold:\n",
    "                pred_labels[sorted_indices[i]] = 1\n",
    "                assigned_nodes.add(src)\n",
    "                assigned_nodes.add(tgt)\n",
    "            if(debug):\n",
    "                print(\"True\")\n",
    "        else:\n",
    "            if(debug):\n",
    "                print(\"False\")\n",
    "\n",
    "    return pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d208b3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################################\n",
    "## Functions to  train and evaluate neural network \n",
    "#################################################################################################\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch_geometric.utils import negative_sampling\n",
    "\n",
    "def pyg_train_link_predictor(\n",
    "    model, train_data, val_data, optimizer, criterion, n_epochs=100, debug = False,\n",
    "    early_stopper = None, scheduler = None, apply_constraints = True\n",
    "):\n",
    "    early_stopper = early_stopper\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        model.train()\n",
    "        random.shuffle(train_data)\n",
    "        for graph in train_data:\n",
    "            optimizer.zero_grad()\n",
    "            out, loss, true = model.forward_graph(graph, criterion = criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if(debug):\n",
    "            if epoch % 10 == 0:\n",
    "                # Eval the model at the end of each Epoch\n",
    "                metrics = pyg_eval_link_predictor(model, val_data, criterion = criterion, apply_constraints = apply_constraints)\n",
    "                print(f\"Epoch: {epoch:03d}, Train Loss: {loss:.3f}, Metrics:\",metrics)\n",
    "                \n",
    "\n",
    "        if early_stopper:\n",
    "            if early_stopper.early_stop(metrics[\"loss\"]):             \n",
    "                break\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def pyg_aggregate_metrics_all(metrics_list, loss_criterion=None):\n",
    "    aggregated_metrics = {}\n",
    "    \n",
    "    aggregated_metrics[\"rouc_auc_score\"] = statistics.mean([metric[\"rouc_auc_score\"] for metric in metrics_list])\n",
    "    \n",
    "    # Aggregate loss if provided\n",
    "    if loss_criterion:\n",
    "        aggregated_metrics[\"loss\"] = torch.mean(torch.stack([metric_[\"loss\"] for metric_ in metrics_list]), dim=0)\n",
    "\n",
    "    \n",
    "    # Aggregate other metrics\n",
    "    metric_keys = [\"@best\", \"@0.5\"]\n",
    "\n",
    "    for key in metric_keys:\n",
    "        aggregated_metrics[key] = {}    \n",
    "        aggregated_metrics[key][\"metrics\"] = aggregate_metrics([metric[key][\"metrics\"] for metric in metrics_list])\n",
    "        \n",
    "        sample_metric = metrics_list[0][key]\n",
    "        if(\"@constraints\" in sample_metric):\n",
    "            aggregated_metrics[key][\"@constraints\"] = {}\n",
    "            aggregated_metrics[key][\"@constraints\"][\"metrics\"] = aggregate_metrics([metric[key][\"@constraints\"][\"metrics\"] for metric in metrics_list])\n",
    "\n",
    "    return aggregated_metrics\n",
    "\n",
    "@torch.no_grad()\n",
    "def pyg_eval_link_predictor(model, data, criterion = None, plot_roc_curve=False, debug = False, \n",
    "                                 apply_constraints=True):\n",
    "    model.eval()\n",
    "    \n",
    "    #computed metrics -> \"acc\", \"precision\", \"recall\", \"tp\", fp\", \"tn\", \"fn\"\n",
    "    metrics_dict = {}\n",
    "    metrics_dict[\"individual_metrics\"] = {}#the metrics for each graph, key=graph_id->value=graph_metrics\n",
    "    metrics_dict[\"aggregated_metrics\"] = {}\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        graph = data[i]\n",
    "        graph_id = data[i].graph_id\n",
    "        tp_total_count = len(data[i].edge_list)\n",
    "        \n",
    "        metrics = {}\n",
    "        out, loss, true = model.forward_graph(graph, criterion = criterion)\n",
    "        if(criterion!=None):\n",
    "            metrics[\"loss\"] = loss\n",
    "        \n",
    "        out = out.sigmoid()\n",
    "        pred = out.cpu().numpy()\n",
    "        \n",
    "        if len(np.unique(pred))==1 or len(np.unique(true)) == 1:\n",
    "            rouc_auc_score = 0\n",
    "        else:\n",
    "            rouc_auc_score = round(roc_auc_score(true, pred), 3)\n",
    "        \n",
    "        metrics[\"rouc_auc_score\"] = rouc_auc_score\n",
    "        fpr, tpr, thresholds = sklearn.metrics.roc_curve(true, pred)\n",
    "        \n",
    "        # 0.5 threshold\n",
    "        pred_labels_05 = (pred > 0.5).astype(int)\n",
    "        metrics[\"@0.5\"] = {}\n",
    "        metrics[\"@0.5\"][\"pred_labels\"] = pred_labels_05#save pred labels to make plot of predicted graph\n",
    "        metrics[\"@0.5\"][\"metrics\"] = eval_metrics(true, pred_labels_05, tp_total_count)\n",
    "\n",
    "        sensitivity = tpr\n",
    "        specificity = 1 - fpr\n",
    "        optimal_idx = np.argmax(sensitivity + specificity - 1)\n",
    "        optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "        # Calculate pred_labels_best with constraints if required\n",
    "        pred_labels_best = (pred > optimal_threshold).astype(int)\n",
    "        metrics[\"@best\"] = {}\n",
    "        metrics[\"@best\"][\"metrics\"] = eval_metrics(true, pred_labels_best, tp_total_count)\n",
    "        metrics[\"@best\"][\"pred_labels\"] = pred_labels_best\n",
    "        metrics[\"@best\"][\"optimal_threshold\"] = optimal_threshold\n",
    "        metrics[\"figures\"] = {}\n",
    "\n",
    "        if(apply_constraints):\n",
    "            pred_labels_constraints_05 = apply_constraints_func(pred, graph.pyg_graph, threshold=0.5)  # Apply constraints\n",
    "            metrics[\"@0.5\"][\"@constraints\"] = {}\n",
    "            metrics[\"@0.5\"][\"@constraints\"][\"pred_labels\"] = pred_labels_constraints_05\n",
    "            metrics[\"@0.5\"][\"@constraints\"][\"metrics\"] = eval_metrics(true, pred_labels_constraints_05, tp_total_count)\n",
    "\n",
    "            pred_labels_constraints_best = apply_constraints_func(pred, graph.pyg_graph, threshold=optimal_threshold)  # Apply constraints\n",
    "            metrics[\"@best\"][\"@constraints\"] = {}\n",
    "            metrics[\"@best\"][\"@constraints\"][\"metrics\"] = eval_metrics(true, pred_labels_constraints_best, tp_total_count)\n",
    "            metrics[\"@best\"][\"@constraints\"][\"pred_labels\"] = pred_labels_constraints_best\n",
    "            \n",
    "            pred_labels_constraints = apply_constraints_func(pred, graph.pyg_graph, threshold=0)  # Apply constraints\n",
    "            metrics[\"@constraints\"] = {}\n",
    "            metrics[\"@constraints\"][\"metrics\"] = eval_metrics(true, pred_labels_constraints, tp_total_count)\n",
    "            metrics[\"@constraints\"][\"pred_labels\"]  = pred_labels_constraints\n",
    "        \n",
    "        metrics[\"pred_edge_probabilities\"] = pred\n",
    "        metrics_dict[\"individual_metrics\"][graph_id]= metrics\n",
    "        data[i].metrics = metrics\n",
    "\n",
    "    metrics_dict[\"aggregated_metrics\"] = pyg_aggregate_metrics_all(list(metrics_dict[\"individual_metrics\"].values()), \n",
    "                                                               loss_criterion = criterion)\n",
    "    \n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b3ccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_type, dataset_num_node_features, dataset_num_edge_features, dataset_num_total_features,\n",
    "               dataset_num_classes):\n",
    "    model = None\n",
    "    \n",
    "    if(model_type==\"GNN_Classifier\"):\n",
    "        param_combos = build_param_combos_gnn(dataset_num_edge_features)\n",
    "        params_gnn = build_params_dict_gnn(*param_combos[0])\n",
    "\n",
    "        model = GraphClassifierGNN(params_gnn)\n",
    "    elif(model_type ==\"MLP\"):\n",
    "        model_dims = (dataset_num_total_features, 100, 100, dataset_num_classes)\n",
    "        model = GraphClassifierMLP(model_dims)\n",
    "    else:\n",
    "        try:\n",
    "            model = GraphClassifierPyg(model_type, \n",
    "                    dataset_num_node_features, 100, 100, dataset_num_classes, \n",
    "                    dataset_num_edge_features,\n",
    "                    decode_type = \"lin\")\n",
    "        except:\n",
    "            raise ValueError(\"Wrong model type!\")\n",
    "        \n",
    "    return model\n",
    "    \n",
    "def schedule_training_GNN(job_parameters, model, graph_list_train, graph_list_val , debug = False):   \n",
    "    \n",
    "    dataset_num_classes = job_parameters[\"num_classes\"]\n",
    "    dataset_num_node_features = job_parameters[\"num_node_features\"]\n",
    "    dataset_num_edge_features = job_parameters[\"num_edge_features\"]\n",
    "    k_inter = job_parameters[\"knn_inter_nodes\"]\n",
    "    k_intra = job_parameters[\"knn_intra_nodes\"]\n",
    "    \n",
    "    lr = job_parameters[\"lr\"]\n",
    "    n_epochs = job_parameters[\"n_epochs\"]\n",
    "    device = job_parameters[\"device\"]\n",
    "    \n",
    "    criterion_switch = {\"BCEWithLogitsLoss\":torch.nn.BCEWithLogitsLoss}\n",
    "    criterion_function = criterion_switch[job_parameters[\"criterion\"]]\n",
    "    \n",
    "    if job_parameters[\"pos_weight\"]:\n",
    "        k_inter_mean = statistics.mean([g.k_inter for g in graph_list_train])\n",
    "        pos_weight = torch.tensor([max((k_inter_mean+k_intra*2-1)/1,1)])\n",
    "        criterion = criterion_function(pos_weight = pos_weight)\n",
    "    else:\n",
    "        criterion = criterion_function()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=lr)\n",
    "    early_stopper = job_parameters[\"early_stopper\"]\n",
    "    scheduler = job_parameters[\"scheduler\"]\n",
    "    \n",
    "    model = pyg_train_link_predictor(model, graph_list_train, graph_list_val, optimizer, criterion,\n",
    "                                          n_epochs=n_epochs, debug = debug, early_stopper = early_stopper, scheduler = scheduler)\n",
    "            \n",
    "    return model         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978f8764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv_groups(data_type):\n",
    "    if data_type in [\"Real\", \"Real_automatic\"]:\n",
    "        cross_validation_groups = [\n",
    "            [\"Crop1.csv\", \"Crop2.csv\", \"Crop3.csv\", \"Crop4.csv\"],\n",
    "            [\"Crop5_BC.csv\", \"Crop6_BC.csv\"],\n",
    "            [\"Crop7_BC.csv\", \"Crop8_BC.csv\"]\n",
    "        ]\n",
    "    else:\n",
    "        cross_validation_groups = \"even\"\n",
    "    return cross_validation_groups\n",
    "\n",
    "def get_job_params_dl(debug = False):\n",
    "    combinations = {\n",
    "         \"data_type_train\":[\n",
    "                    \"Real\"\n",
    "        ],\n",
    "        \"data_type_test\":[\n",
    "            \"Real\",\n",
    "             #\"Real_automatic\",\n",
    "        ],\n",
    "        \"model_type\":[\n",
    "                    \"GNN_Classifier\", \n",
    "                    #\"MLP\",\n",
    "                    ],\n",
    "        \"knn_inter_nodes\":[\n",
    "                            #7,\n",
    "                            10\n",
    "                            #\"min\"\n",
    "                        ],\n",
    "        \"knn_inter_nodes_max\": [7],\n",
    "       \"knn_intra_nodes\":[0],\n",
    "        \"normalize\":[True],#False],\n",
    "        \"node_feats\":[\n",
    "        [\n",
    "            #'Y', \n",
    "            #'X', \n",
    "            #'Z', \n",
    "            'node_type', \n",
    "            #'ID'\n",
    "        ],\n",
    "        #[\n",
    "        #    'Y', \n",
    "        #    'X', \n",
    "        #    'Z', \n",
    "        #    'node_type',\n",
    "        #]\n",
    "        ],\n",
    "\n",
    "        \"edge_feats\":[[\n",
    "     'delta_x',\n",
    "     'delta_y',\n",
    "     'delta_z',\n",
    "     'weight',\n",
    "     'angle_orientation_theta',\n",
    "     'angle_orientation_phi'],\n",
    "            [\n",
    "     'delta_x',\n",
    "     'delta_y',\n",
    "     'delta_z',\n",
    "     'weight',\n",
    "     ],\n",
    "      #[\n",
    "     #'angle_orientation_theta',\n",
    "     #'angle_orientation_phi']\n",
    "     ],\n",
    "\n",
    "        \"to_undirected\":[False],\n",
    "       \"lr\":[1e-3],\n",
    "       \"n_epochs\":[100],\n",
    "        \"early_stopper\": [None],\n",
    "        \"scheduler\" : [None],\n",
    "        \"pos_weight\" : [True],\n",
    "        \"criterion\" : [\"BCEWithLogitsLoss\"],\n",
    "        \"device\" : [\"cpu\"]\n",
    "    }\n",
    "    \n",
    "    jobs = []\n",
    "    \n",
    "    # Generate all possible combinations of the dictionary values\n",
    "    for values in itertools.product(*combinations.values()):\n",
    "        # Generate a dictionary for the combination of values\n",
    "        job_dict = dict(zip(combinations.keys(), values))\n",
    "        job_dict[\"scale_features\"] = True if \"Real\" in job_dict[\"data_type_train\"] else False\n",
    "\n",
    "        index_train = \"all\"\n",
    "        index_test = \"all\"\n",
    "        cross_validation_groups_train = get_cv_groups(job_dict[\"data_type_train\"])\n",
    "        cross_validation_groups_test = get_cv_groups(job_dict[\"data_type_test\"])\n",
    "\n",
    "        job_dict[\"index_train\"] = index_train\n",
    "        job_dict[\"index_test\"] = index_test\n",
    "        job_dict[\"cross_validation_groups_train\"] = cross_validation_groups_train\n",
    "        job_dict[\"cross_validation_groups_test\"] = cross_validation_groups_test\n",
    "\n",
    "        jobs.append(job_dict)\n",
    "    \n",
    "    if(debug):\n",
    "        print(\"Total Number of jobs is:\",len(jobs))\n",
    "        print(json.dumps(jobs))\n",
    "    return jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb70f9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_list_dl(jobs, debug = False):\n",
    "    #build dataframes\n",
    "    graph_list_dict_deep_learning = {}\n",
    "\n",
    "    for params in tqdm(jobs):\n",
    "\n",
    "        params_list_train = [params[\"data_type_train\"], params[\"knn_inter_nodes\"], params[\"knn_intra_nodes\"], \n",
    "                                        params[\"knn_inter_nodes_max\"], params[\"normalize\"],\n",
    "                                        params[\"scale_features\"], str(params[\"node_feats\"]), str(params[\"edge_feats\"])]\n",
    "        params_list_train = [str(param_) for param_ in params_list_train]\n",
    "        graph_key = \"_\".join(params_list_train)\n",
    "\n",
    "        if graph_key not in graph_list_dict_deep_learning:\n",
    "            graph_list = get_graph_list(params[\"data_type_train\"], params[\"knn_inter_nodes\"], params[\"knn_intra_nodes\"], \n",
    "                                            params[\"knn_inter_nodes_max\"],  normalize = params[\"normalize\"],\n",
    "                                            scale_feats = params[\"scale_features\"],\n",
    "                                            node_feats = params[\"node_feats\"], edge_feats = params[\"edge_feats\"],\n",
    "                                            shuffle = False)\n",
    "            graph_list_dict_deep_learning[graph_key] = graph_list\n",
    "\n",
    "        params_list_test = [params[\"data_type_test\"], params[\"knn_inter_nodes\"], params[\"knn_intra_nodes\"], \n",
    "                                        params[\"knn_inter_nodes_max\"], params[\"normalize\"],\n",
    "                                        params[\"scale_features\"], str(params[\"node_feats\"]), str(params[\"edge_feats\"])]\n",
    "        params_list_test = [str(param_) for param_ in params_list_test]\n",
    "        graph_key = \"_\".join(params_list_test)\n",
    "\n",
    "        if graph_key not in graph_list_dict_deep_learning:\n",
    "            graph_list = get_graph_list(params[\"data_type_test\"], params[\"knn_inter_nodes\"], params[\"knn_intra_nodes\"], \n",
    "                                            params[\"knn_inter_nodes_max\"], normalize = params[\"normalize\"],\n",
    "                                            scale_feats = params[\"scale_features\"],\n",
    "                                            node_feats = params[\"node_feats\"], edge_feats = params[\"edge_feats\"],\n",
    "                                            shuffle = False)\n",
    "            graph_list_dict_deep_learning[graph_key] = graph_list\n",
    "    return graph_list_dict_deep_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83f6bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dl(graph_list_dict_deep_learning, jobs, debug = False):\n",
    "    results_list_pytorch = []\n",
    "\n",
    "    for i, job_parameters in tqdm(enumerate(jobs), total=len(jobs)):\n",
    "\n",
    "        k_inter = job_parameters[\"knn_inter_nodes\"]\n",
    "        k_inter_max = job_parameters[\"knn_inter_nodes_max\"]\n",
    "        k_intra = job_parameters[\"knn_intra_nodes\"]\n",
    "\n",
    "        scale_feats = job_parameters[\"scale_features\"]\n",
    "\n",
    "        #get data\n",
    "        params_list_train = [job_parameters[\"data_type_train\"], job_parameters[\"knn_inter_nodes\"], job_parameters[\"knn_intra_nodes\"], \n",
    "                                        job_parameters[\"knn_inter_nodes_max\"], job_parameters[\"normalize\"],\n",
    "                                        job_parameters[\"scale_features\"],\n",
    "                                         str(job_parameters[\"node_feats\"]), str(job_parameters[\"edge_feats\"])]\n",
    "        params_list_train = [str(param_) for param_ in params_list_train]\n",
    "        graph_key = \"_\".join(params_list_train)\n",
    "        graph_list_train = graph_list_dict_deep_learning[graph_key]\n",
    "\n",
    "        params_list_test = [job_parameters[\"data_type_test\"], job_parameters[\"knn_inter_nodes\"], job_parameters[\"knn_intra_nodes\"], \n",
    "                                        job_parameters[\"knn_inter_nodes_max\"], job_parameters[\"normalize\"],\n",
    "                                        job_parameters[\"scale_features\"],\n",
    "                                         str(job_parameters[\"node_feats\"]), str(job_parameters[\"edge_feats\"])]\n",
    "        params_list_test = [str(param_) for param_ in params_list_test]\n",
    "        graph_key = \"_\".join(params_list_test)\n",
    "        graph_list_test = graph_list_dict_deep_learning[graph_key]\n",
    "\n",
    "        job_parameters[\"num_classes\"] = 1\n",
    "        job_parameters[\"num_node_features\"] = graph_list_train[0].pyg_graph.x.shape[1] if graph_list_train[0].pyg_graph.x.shape[0] >0 else 0\n",
    "        job_parameters[\"num_edge_features\"] = graph_list_train[0].pyg_graph.edge_attr.shape[1]\n",
    "        job_parameters[\"num_total_features\"] = graph_list_train[0].edge_x.shape[1]\n",
    "\n",
    "        indexes_train = job_parameters[\"index_train\"]\n",
    "        indexes_test = job_parameters[\"index_test\"]\n",
    "        cross_validation_groups_train = job_parameters.get(\"cross_validation_groups_train\",[])\n",
    "        cross_validation_groups_test = job_parameters.get(\"cross_validation_groups_test\",[])\n",
    "\n",
    "        if(indexes_train==\"all\"):\n",
    "            indexes_train = [g.graph_id for g in graph_list_train]\n",
    "        if(indexes_test==\"all\"):\n",
    "            indexes_test = [g.graph_id for g in graph_list_test]\n",
    "        if(cross_validation_groups_train==\"even\"):\n",
    "            number_cross_validation_groups = 3\n",
    "            cross_validation_groups_train = distribute_elements_to_lists(indexes_train, number_cross_validation_groups)\n",
    "        if(cross_validation_groups_test==\"even\"):\n",
    "            number_cross_validation_groups = 3\n",
    "            cross_validation_groups_test = distribute_elements_to_lists(indexes_test, number_cross_validation_groups)\n",
    "\n",
    "        indexes_train = set(indexes_train)\n",
    "        indexes_test = set(indexes_test)\n",
    "\n",
    "        cv_dataset_list = []\n",
    "        if(not cross_validation_groups_train):#without cross-validation\n",
    "            graph_list_train = [el for el in graph_list if el.graph_id in indexes_train]\n",
    "            graph_list_test = [el for el in graph_list if el.graph_id in indexes_test]\n",
    "            cv_dataset_list.append({\"train\":graph_list_train, \"test\":graph_list_test})\n",
    "\n",
    "        else:#with cross-valudation\n",
    "            for i in range(len(cross_validation_groups_test)):\n",
    "                graph_list_test_cv = [el for el in graph_list_test if el.graph_id in set(cross_validation_groups_test[i])]\n",
    "                graph_list_train_cv = []\n",
    "                for j in range(len(cross_validation_groups_train)):\n",
    "                    if(j!=i):\n",
    "                        graph_list_train_cv.extend([el for el in graph_list_train if el.graph_id in set(cross_validation_groups_train[j])])\n",
    "\n",
    "                cv_dataset_list.append({\"train\": graph_list_train_cv, \"test\": graph_list_test_cv})    \n",
    "\n",
    "        #Train Model\n",
    "        results = {\"cv_results\":[], \"job_parameters\":job_parameters, \"aggregated_metrics\" : None}\n",
    "\n",
    "        for dataset in cv_dataset_list:\n",
    "\n",
    "            result = {}\n",
    "            graph_list_train, graph_list_test = dataset[\"train\"], dataset[\"test\"]\n",
    "\n",
    "            model_type = job_parameters[\"model_type\"]\n",
    "            model = build_model(model_type, job_parameters[\"num_node_features\"], job_parameters[\"num_edge_features\"], \n",
    "                                job_parameters[\"num_total_features\"], 1)\n",
    "\n",
    "            model = schedule_training_GNN(job_parameters, model, \n",
    "                                               graph_list_train, graph_list_test, debug = debug)\n",
    "            \n",
    "            result[\"graphs\"] = {}\n",
    "            result[\"graphs\"][\"train\"] = graph_list_train\n",
    "            result[\"graphs\"][\"test\"] = graph_list_test\n",
    "\n",
    "            #Eval Model\n",
    "            result[\"eval\"] = pyg_eval_link_predictor(model, graph_list_test, \n",
    "                                                     criterion = None,  apply_constraints = True,\n",
    "                                                    plot_roc_curve = False, debug = False)\n",
    "            results[\"cv_results\"].append(result)\n",
    "\n",
    "\n",
    "        #aggregate all metrics\n",
    "\n",
    "        all_metrics = {}\n",
    "        for item in results[\"cv_results\"]:\n",
    "            individual_metrics = item[\"eval\"][\"individual_metrics\"]\n",
    "            for individual_graph in individual_metrics:\n",
    "                all_metrics[individual_graph] = individual_metrics[individual_graph]\n",
    "\n",
    "        results[\"aggregated_metrics\"]  = pyg_aggregate_metrics_all(list(all_metrics.values()))\n",
    "\n",
    "        results_list_pytorch.append(results)\n",
    "    return results_list_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca17617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_dl(results_list_pytorch):\n",
    "    plot_df_pytorch = plot_table(results_list_pytorch, metrics_dict_entries = [[\"@best\",\"metrics\"],[\"@best\",\"@constraints\",\"metrics\"]])\n",
    "    plot_df_pytorch = plot_df_pytorch.sort_values(by=[\"Algorithm\", \"Normalize\", \"K Inter\", 'Data Train', 'Data Test','Constraints'])\n",
    "    display(plot_df_pytorch)\n",
    "    plot_df_pytorch = plot_df_pytorch.drop([\"Data Train\", \"Data Test\"], axis=1)\n",
    "    display(plot_df_pytorch)\n",
    "    print(plot_df_to_latex(plot_df_pytorch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ae5b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_dl = get_job_params_dl()\n",
    "len(jobs_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c366c910",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_list_dl = get_graph_list_dl(jobs_dl, debug = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea890e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list_dl = train_dl(graph_list_dl, jobs_dl, debug = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e26bdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "plot_results_dl(results_list_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dc9bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annote_figure(figax, annotations, dims, fontsize = 7):\n",
    "    \n",
    "    start_point_y = 0.9\n",
    "    \n",
    "    if(dims ==2):\n",
    "        start_point_x = 1.05\n",
    "    else:\n",
    "        start_point_x = 1.2\n",
    "        \n",
    "    for i, annotation in enumerate(annotations):\n",
    "        figax.annotate(annotation, xy=(start_point_x, start_point_y - i * 0.1), \n",
    "                       xycoords=\"axes fraction\", fontsize=fontsize, color=\"black\")\n",
    "    return\n",
    "\n",
    "def annote_knn_metrics(figax, knn_intra, knn_inter, dims):\n",
    "    knn_annotations = [\n",
    "        r'$K_{{INTRA\\_CLASS}}: {}$'.format(knn_intra),\n",
    "        r'$K_{{INTER\\_CLASS}}: {}$'.format(knn_inter),\n",
    "    ]\n",
    "    \n",
    "    annote_figure(figax, knn_annotations, dims)\n",
    "    return \n",
    "\n",
    "def annote_figure_results_metrics(figax, metrics, threshold, dims):\n",
    "    \n",
    "    metrics = metrics[\"metrics\"]\n",
    "\n",
    "    count_annotations = [\n",
    "        f\"Threshold: {threshold:.3f}\",\n",
    "        f\"Acc.: {metrics['acc']}\",\n",
    "        f\"Prec.: {metrics['precision']}\",\n",
    "        f\"Recall: {metrics['recall']}\",\n",
    "        f\"TP: {metrics['tp']}\",\n",
    "        f\"FP: {metrics['fp']}\",\n",
    "        f\"TN: {metrics['tn']}\",\n",
    "        f\"FN: {metrics['fn']}\"\n",
    "    ]\n",
    "    \n",
    "    annote_figure(figax, count_annotations, dims)\n",
    "    return\n",
    "\n",
    "def plot_graph_results(graph, graph_metrics, k_intra, k_inter):\n",
    "    plot_styles = {\n",
    "                \"nuclei\":{\"marker\":\"o\",\"color\":\"red\", \"alpha\":0.3},\n",
    "                \"golgi\":{\"marker\":\"o\",\"color\":\"green\", \"alpha\":0.3},\n",
    "                \"tp\": {\"color\": \"black\",  \"dashed\": False, \"alpha\":1},\n",
    "                \"fp\": {\"color\": \"yellow\", \"dashed\": False, \"alpha\":1},\n",
    "                \"tn\": None,\n",
    "                \"fn\": {\"color\": \"red\",  \"dashed\": False, \"alpha\":1}\n",
    "    }\n",
    "    dims = 3\n",
    "    subplot_params = {\"projection\":\"3d\"} if dims==3 else {}\n",
    "\n",
    "    node_list_true = graph.node_list\n",
    "    edge_list_true = graph.edge_list\n",
    "    edge_list_knn = graph.pyg_graph_edge_list\n",
    "    true_labels = graph.pyg_graph_true_labels\n",
    "\n",
    "    metrics_best = graph_metrics[\"@best\"]\n",
    "    optimal_threshold = metrics_best[\"optimal_threshold\"]\n",
    "    metrics_constraints = graph_metrics[\"@constraints\"]\n",
    "    metrics_constraints_threshold = graph_metrics[\"@best\"][\"@constraints\"]\n",
    "    pred_edge_probabilities = graph_metrics[\"pred_edge_probabilities\"]\n",
    "\n",
    "    fig_all = plt.figure(figsize=(18, 9), dpi= 300)#12,9\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.15)#wspace-> horizontal hspace-> vertical\n",
    "\n",
    "    #True Graph\n",
    "    figax = plt.subplot(2, 3, 1, **subplot_params)\n",
    "    plot_edge_labels = [\"tp\"]*len(edge_list_true)\n",
    "    true_graph_fig = GraphInfo.make_graph_plot(node_list_true, edge_list_true, \n",
    "                                            plot_edge_labels, plot_styles, dims = dims,\n",
    "                                     title = \"(a) True\", figax = figax)\n",
    "\n",
    "    #KNN Graph\n",
    "    figax = plt.subplot(2, 3 , 2 , **subplot_params)\n",
    "\n",
    "    plot_edge_labels = [\"tp\"]*len(edge_list_knn)\n",
    "    knn_graph_fig = GraphInfo.make_graph_plot(node_list_true, edge_list_knn, \n",
    "                                              plot_edge_labels, plot_styles, dims = dims, \n",
    "                                    title = \"(b) KNN\", figax = figax)\n",
    "    annote_knn_metrics(knn_graph_fig, k_intra, k_inter, dims)\n",
    "\n",
    "    #Pred Best Threshold\n",
    "    figax = plt.subplot(2, 3, 3, **subplot_params)\n",
    "    metrics = metrics_best\n",
    "    edge_list_pred_labels_best = metrics[\"pred_labels\"]\n",
    "    edge_list_pred_best = GraphInfo.edge_index_to_edge_list(graph.pyg_graph.edge_index)\n",
    "    plot_edge_labels_pred_best = GraphInfo.convert_edge_preds_to_labels(true_labels, edge_list_pred_labels_best)\n",
    "    \n",
    "    predicted_graph_fig = GraphInfo.make_graph_plot(node_list_true,  edge_list_pred_best, \n",
    "                                            plot_edge_labels_pred_best, plot_styles, dims = dims, \n",
    "                                          title = \"(c) Pred. W/ Best Threshold\", figax = figax)\n",
    "\n",
    "    annote_figure_results_metrics(figax, metrics, optimal_threshold, dims)\n",
    "\n",
    "    #Pred Constraints With Threshold\n",
    "    figax = plt.subplot(2, 3, 4, **subplot_params)\n",
    "    metrics = metrics_constraints_threshold\n",
    "    edge_list_pred_labels_constraints_threshold = metrics[\"pred_labels\"]\n",
    "    edge_list_pred_constraints_threshold = GraphInfo.edge_index_to_edge_list(graph.pyg_graph.edge_index)\n",
    "    plot_edge_labels_pred_constraints_threshold = GraphInfo.convert_edge_preds_to_labels(true_labels, edge_list_pred_labels_constraints_threshold)\n",
    "   \n",
    "    predicted_graph_fig = GraphInfo.make_graph_plot(node_list_true, edge_list_pred_constraints_threshold, \n",
    "                                        plot_edge_labels_pred_constraints_threshold,  plot_styles, dims = dims, \n",
    "                                          title = \"(d) Pred. W/ Best Threshold W/ Constraints\", figax = figax)\n",
    "\n",
    "    annote_figure_results_metrics(figax, metrics, optimal_threshold , dims)\n",
    "\n",
    "    #Pred Constraints Without Threshold\n",
    "    figax = plt.subplot(2, 3, 5, **subplot_params)\n",
    "    metrics = metrics_constraints\n",
    "    edge_list_pred_labels_constraints = metrics[\"pred_labels\"]\n",
    "    edge_list_pred_constraints = GraphInfo.edge_index_to_edge_list(graph.pyg_graph.edge_index)\n",
    "    plot_edge_labels_pred_constraints = GraphInfo.convert_edge_preds_to_labels(true_labels, edge_list_pred_labels_constraints)\n",
    " \n",
    "    predicted_graph_fig = GraphInfo.make_graph_plot(node_list_true, edge_list_pred_constraints, \n",
    "                                        plot_edge_labels_pred_constraints,  plot_styles, dims = dims, \n",
    "                                          title = \"(e) Pred. W/ Constraints W/o Threshold\", figax = figax)\n",
    "\n",
    "    annote_figure_results_metrics(figax, metrics, 0 , dims)\n",
    "\n",
    "    ########################\n",
    "    # Fig All legends ######\n",
    "    ########################\n",
    "\n",
    "    # Create custom legend handles and labels based on plot_styles\n",
    "    legend_handles = []\n",
    "    legend_labels = []\n",
    "\n",
    "    #Add Edge Legends\n",
    "    for label, style in plot_styles.items():\n",
    "        if isinstance(style, dict) and \"dashed\" in style:\n",
    "            color = style[\"color\"]\n",
    "            dashed = style.get(\"dashed\", False)\n",
    "            alpha = style[\"alpha\"]\n",
    "            linestyle = \"--\" if dashed else \"-\"\n",
    "            legend_handles.append(matplotlib.lines.Line2D([0], [0], color=color, linewidth=2, linestyle=linestyle, label=label, alpha = alpha))\n",
    "            legend_labels.append(label.upper() + \" Edge\")\n",
    "\n",
    "    # Add node legends\n",
    "    legend_handles.append(matplotlib.lines.Line2D([0], [0], marker=plot_styles[\"nuclei\"][\"marker\"], color=\"w\", alpha = plot_styles[\"nuclei\"][\"alpha\"],\n",
    "                                    label=\"Nuclei\", markerfacecolor=plot_styles[\"nuclei\"][\"color\"], markersize=10))\n",
    "    legend_labels.append(\"Nuclei\")\n",
    "    legend_handles.append(matplotlib.lines.Line2D([0], [0], marker=plot_styles[\"golgi\"][\"marker\"], color=\"w\", alpha = plot_styles[\"golgi\"][\"alpha\"],\n",
    "                                    label=\"Golgi\", markerfacecolor=plot_styles[\"golgi\"][\"color\"], markersize=10))\n",
    "    legend_labels.append(\"Golgi\")\n",
    "\n",
    "    legend = fig_all.legend(legend_handles, legend_labels, loc=\"upper right\", fontsize=\"small\", ncol=2)\n",
    "\n",
    "    legend.set_bbox_to_anchor((0.75, 0.15))\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    return\n",
    "\n",
    "def describe_results(results_list_pytorch, make_plot = False):\n",
    "    results_entry = results_list_pytorch[0]\n",
    "\n",
    "    k_intra = results_entry[\"job_parameters\"][\"knn_intra_nodes\"]\n",
    "    k_inter = results_entry[\"job_parameters\"][\"knn_inter_nodes\"]\n",
    "\n",
    "    print(\"K_Intra\", k_intra, \"K_Inter\", k_inter)\n",
    "\n",
    "    print(\"Aggregated Metrics\", json.dumps(results_entry[\"aggregated_metrics\"], indent = 1, cls = CustomEncoder))\n",
    "\n",
    "    for cv_crop_index, cv_crop in enumerate(results_entry[\"cv_results\"]):\n",
    "        graphs_list_test = cv_crop[\"graphs\"][\"test\"]\n",
    "        graphs_list_train = cv_crop[\"graphs\"][\"train\"]\n",
    "        graph_individual_metrics = cv_crop[\"eval\"][\"individual_metrics\"]\n",
    "        graphs_indexes = {\"train\":[g.graph_id for g in graphs_list_train], \"test\":[g.graph_id for g in graphs_list_test]}\n",
    "\n",
    "        print(\"##############################################################\\n\")\n",
    "        print(\"CV Crop Index\",cv_crop_index, \"\", graphs_indexes)\n",
    "        #print(json.dumps(cv_crop[\"eval\"][\"aggregated_metrics\"], indent = 1, cls = CustomEncoder))\n",
    "\n",
    "        for graph_id in graphs_indexes[\"test\"]:\n",
    "            graph_matches = [g for g in graphs_list_test if g.graph_id ==graph_id]\n",
    "            if len(graph_matches)!=1:\n",
    "                raise ValueError(\"Multiple graphs with same ID!\")\n",
    "            graph_test = graph_matches[0]\n",
    "            graph_metrics = graph_individual_metrics[graph_id]\n",
    "\n",
    "            if make_plot:\n",
    "                print(graph_id)\n",
    "\n",
    "                #print(json.dumps(graph_metrics, cls = CustomEncoder))\n",
    "                plot_graph_results(graph_test, graph_metrics, k_intra, k_inter)\n",
    "        print(\"##############################################################\\n\")\n",
    "\n",
    "describe_results(results_list_dl, make_plot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e7c875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(output_folder, results_list_pytorch):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    results_count = 0\n",
    "    #Save results to file\n",
    "    for results_entry in results_list_pytorch:\n",
    "        results_entry_output_folder = os.path.join(output_folder, \"Results_\"+str(results_count))\n",
    "        if not os.path.exists(results_entry_output_folder):\n",
    "            os.makedirs(results_entry_output_folder)\n",
    "            \n",
    "        desc_file_path = os.path.join(results_entry_output_folder, \"params.json\")\n",
    "        with open(desc_file_path, 'w') as f:\n",
    "            json.dump(results_entry[\"job_parameters\"], f, indent = 2)\n",
    "        \n",
    "        data_type_train = results_entry[\"job_parameters\"][\"data_type_train\"]\n",
    "        data_type_test = results_entry[\"job_parameters\"][\"data_type_test\"]\n",
    "        model_type = results_entry[\"job_parameters\"][\"model_type\"]\n",
    "        k_intra = results_entry[\"job_parameters\"][\"knn_intra_nodes\"]\n",
    "        k_inter = results_entry[\"job_parameters\"][\"knn_inter_nodes\"]\n",
    "        node_feats_str = str(results_entry[\"job_parameters\"][\"node_feats\"]).replace(\"[\",\"\").replace(\",\",\"_\").replace(\"]\",\"\").replace(\"\\'\",\"\")\n",
    "        edge_feats_str = str(results_entry[\"job_parameters\"][\"edge_feats\"]).replace(\"[\",\"\").replace(\",\",\"_\").replace(\"]\",\"\").replace(\"\\'\",\"\")\n",
    "\n",
    "        print(\"K_Intra\", k_intra, \"K_Inter\", k_inter)\n",
    "\n",
    "        print(\"Aggregated Metrics\", json.dumps(results_entry[\"aggregated_metrics\"], indent = 1, cls = CustomEncoder))\n",
    "\n",
    "        for cv_crop_index, cv_crop in enumerate(results_entry[\"cv_results\"]):\n",
    "            graphs_list_test = cv_crop[\"graphs\"][\"test\"]\n",
    "            graphs_list_train = cv_crop[\"graphs\"][\"train\"]\n",
    "            graph_individual_metrics = cv_crop[\"eval\"][\"individual_metrics\"]\n",
    "            graphs_indexes = {\"train\":[g.graph_id for g in graphs_list_train], \"test\":[g.graph_id for g in graphs_list_test]}\n",
    "\n",
    "            print(\"##############################################################\\n\")\n",
    "            print(\"CV Crop Index\",cv_crop_index, \"\", graphs_indexes)\n",
    "\n",
    "            for graph_id in graphs_indexes[\"test\"]:\n",
    "                graph_matches = [g for g in graphs_list_test if g.graph_id ==graph_id]\n",
    "                if len(graph_matches)!=1:\n",
    "                    raise ValueError(\"Multiple graphs with same ID!\")\n",
    "                graph_test = graph_matches[0]\n",
    "                graph_metrics = graph_individual_metrics[graph_id]\n",
    "\n",
    "                edge_list = GraphInfo.edge_index_to_edge_list(graph_test.pyg_graph.edge_index)\n",
    "                edge_df = GraphInfo.edge_list_to_edge_df(edge_list)\n",
    "                #nodes_df = graph_test.nodes_df\n",
    "\n",
    "                edge_df_constraints = edge_df.copy()\n",
    "                edge_df_constraints[\"edge_label\"] = graph_metrics[\"@constraints\"][\"pred_labels\"]            \n",
    "                constraints_array = pred_df_to_csv(edge_df_constraints, graph_test.nodes_df_original)\n",
    "\n",
    "                output_file_path = os.path.join(results_entry_output_folder, graph_test.graph_id)\n",
    "                array_to_csv(constraints_array, output_file_path)\n",
    "                \n",
    "        results_count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8076f478",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(os.path.join(\"./results\",\"results_automatic_test\"), results_list_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248b2ada",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
